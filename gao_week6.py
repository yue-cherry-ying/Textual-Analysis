from sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.metrics.pairwise import cosine_similarityimport numpyimport refrom nltk.corpus import stopwords# convert text to a dictionary that includes chapter name and text as string# Return a list of tupledef get_chapters(filename):    chapters = {}    current_chapter = ""    with open(filename) as input_file:        for line in input_file:            line = line.strip()            if re.match(r"CHAPTER ", line) or re.match(r"Chapter ", line) or re.match(r"Letter ", line):                current_chapter = line                if current_chapter not in chapters:                    chapters[current_chapter] = ""                continue            if current_chapter == "":                continue            words = re.findall(r"\w+", line)            chapters[current_chapter] += str(words)    list_tuple = [(str(filename), chapter, text) for chapter, text in chapters.items()]    return list_tuple    def read_and_tokenize_doc(text_contents, english_stopwords):    tokens = re.findall(r"\w+", text_contents)    filtered_tokens = []    for token in tokens:        if token not in english_stopwords and re.search(r"[!,?;.:]", token) is None:            filtered_tokens.append(token)    filtered_text = " ".join(filtered_tokens)    return filtered_textdef vectorization(documents):    clean_docs = [read_and_tokenize_doc(doc[2], english_stopwords) for doc in documents]    vectorizer = TfidfVectorizer(min_df=2)    corpus_vectorized = vectorizer.fit_transform(clean_docs)    results = cosine_similarity(corpus_vectorized)    for doc_pos, result_vector in enumerate(results):        outer_doc_name = documents[doc_pos][0] + " " + documents[doc_pos][1]        for doc_index in numpy.argsort(result_vector)[::-1]:  # argsort returns the index positions            inner_doc_name = documents[doc_index][0] + " " + documents[doc_index][1]            score = result_vector[doc_index]            if inner_doc_name != outer_doc_name:                print(f"the most similar chapter to {outer_doc_name} is {inner_doc_name} with a score of {score}" + "\n")                break        breakif __name__ == "__main__":    english_stopwords = set(stopwords.words("english"))    # Get Chapter 1 in Pride and Prejudice    C1_pride = []    for item in get_chapters("pride_and_prejudice.txt"):        C1_pride.append(item)        break    # Create a list of tuple    list_dracula = get_chapters("dracula.txt")    list_frankenstein = get_chapters("frankenstein.txt")    list_GreatExpectations = get_chapters("great_expectations.txt")    # Add all tuples together in a list where the first tuple in the first Chapter of Pride and Prejudice    overall_documents = C1_pride + list_GreatExpectations + list_frankenstein + list_dracula    pride_dracula = C1_pride + list_dracula    pride_frankenstein = C1_pride + list_frankenstein    pride_great = C1_pride + list_GreatExpectations    print("In Dracula,")    vectorization(pride_dracula)    print("In Frankenstein,")    vectorization(pride_frankenstein)    print("In Great Expectations,")    vectorization(pride_great)    print("Overall,")    vectorization(overall_documents)